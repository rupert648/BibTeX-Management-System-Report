@manual{bibtexOfficialDocumentation,
    title={Bib{T}e{X}ing},
    author={Oren Patashnik},
    organization={CTAN.org},
    month={February},
    year={1988}
}

@manual{bibtexSpecialCharacters,
    title={BibTeX Special Symbols and Escapes},
    author={Alexander Feder},
    organization={BibTeX.org},
    year={2006}
}

@misc{papersapp,
    author={{PapersApp Core Team}},
    title={papersapp},
    organization={papersapp.com},
    url={https://www.papersapp.com/}
}

@misc{zotero,
    author={{Zotero Core Team}},
    title={Zotero},
    organization={Zotero.org},
    url={https://www.zotero.org/}
}

@misc{mendeley,
    author={{Mendeley Core Team}},
    title={Mendeley},
    organization={Mendeley.com},
    url={https://www.mendeley.com/reference-management/mendeley-desktop}
}

@manual{electron,
    title={ElectronJS},
    author={{Electron Core Team}},
    organization={electronjs.org},
    url={https://www.electronjs.org/},
    year=2022
}


@misc{neon,
    author={{Neon Core Team}},
    title={Neon-Bindings},
    organization={neon-bindings.com},
    url={https://neon-bindings.com/}
}

@article{approximateStringMatching,
	title = {A guided tour to approximate string matching},
	volume = {33},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/375360.375365},
	doi = {10.1145/375360.375365},
	abstract = {We survey the current techniques to cope with the problem of string matching that allows errors. This is becoming a more and more relevant issue for many fast growing areas such as information retrieval and computational biology. We focus on online searching and mostly on edit distance, explaining the problem and its relevance, its statistical behavior, its history and current developments, and the central ideas of the algorithms and their complexities. We present a number of experiments to compare the performance of the different algorithms and show which are the best choices. We conclude with some directions for future work and open problems.},
	number = {1},
	urldate = {2022-03-19},
	journal = {ACM Computing Surveys},
	author = {Navarro, Gonzalo},
	month = mar,
	year = {2001},
	keywords = {edit distance, online string matching, text searching allowing errors, Levenshtein distance},
	pages = {31--88},
}

@article{rustProgrammingLanguage,
	title = {The rust language},
	volume = {34},
	issn = {1094-3641},
	url = {https://doi.org/10.1145/2692956.2663188},
	doi = {10.1145/2692956.2663188},
	abstract = {Rust is a new programming language for developing reliable and efficient systems. It is designed to support concurrency and parallelism in building applications and libraries that take full advantage of modern hardware. Rust's static type system is safe1 and expressive and provides strong guarantees about isolation, concurrency, and memory safety. Rust also offers a clear performance model, making it easier to predict and reason about program efficiency. One important way it accomplishes this is by allowing fine-grained control over memory representations, with direct support for stack allocation and contiguous record storage. The language balances such controls with the absolute requirement for safety: Rust's type system and runtime guarantee the absence of data races, buffer overflows, stack overflows, and accesses to uninitialized or deallocated memory.},
	number = {3},
	urldate = {2022-03-20},
	journal = {ACM SIGAda Ada Letters},
	author = {Matsakis, Nicholas D. and Klock, Felix S.},
	month = oct,
	year = {2014},
	keywords = {memory management, systems programming, affine type systems, rust},
	pages = {103--104},
}

@techreport{microsoftRustSafety,
    title       = "Trends, Challenges, and Strategic Shifts in the software vulnerability mitigation landscape",
      author      = "Matt Miller",
      institution = "Microsoft Security Response Center (MSRC)",
      address     = "BlueHat Illinois",
      year        = 2019,
      month       = feb
}

@article{rustbelt,
	title = {{RustBelt}: securing the foundations of the {Rust} programming language},
	volume = {2},
	shorttitle = {{RustBelt}},
	url = {https://doi.org/10.1145/3158154},
	doi = {10.1145/3158154},
	abstract = {Rust is a new systems programming language that promises to overcome the seemingly fundamental tradeoff between high-level safety guarantees and low-level control over resource management. Unfortunately, none of Rust's safety claims have been formally proven, and there is good reason to question whether they actually hold. Specifically, Rust employs a strong, ownership-based type system, but then extends the expressive power of this core type system through libraries that internally use unsafe features. In this paper, we give the first formal (and machine-checked) safety proof for a language representing a realistic subset of Rust. Our proof is extensible in the sense that, for each new Rust library that uses unsafe features, we can say what verification condition it must satisfy in order for it to be deemed a safe extension to the language. We have carried out this verification for some of the most important libraries that are used throughout the Rust ecosystem.},
	number = {POPL},
	urldate = {2022-03-20},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Jung, Ralf and Jourdan, Jacques-Henri and Krebbers, Robbert and Dreyer, Derek},
	month = dec,
	year = {2017},
	keywords = {separation logic, logical relations, Rust, type systems, concurrency},
	pages = {66:1--66:34},
}

@incollection{hammingDefinition,
title = {14 - Coding},
editor = {Fraidoon Mazda},
booktitle = {Telecommunications Engineer's Reference Book},
publisher = {Butterworth-Heinemann},
pages = {14-1-14-13},
year = {1993},
isbn = {978-0-7506-1162-6},
doi = {https://doi.org/10.1016/B978-0-7506-1162-6.50020-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780750611626500204},
author = {M D Macleod}
}

@article{levenshtein1965binary,
  title={Binary codes with correction of deposition, insertions, and substitutions of characters},
  author={Levenshtein, VI},
  journal={Doklady Akademij Nauk SSSR},
  pages={845--848},
  year={1965}
}

@manual{rustNeonDocs,
  title        = "Rust Neon Docs",
  author       = "{Neon Core Team}",
  organization = "neon-bindings",
  year         = 2020
}

@article{wagnerFischer,
author = {Wagner, Robert A. and Fischer, Michael J.},
title = {The String-to-String Correction Problem},
year = {1974},
issue_date = {Jan. 1974},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {0004-5411},
url = {https://doi.org/10.1145/321796.321811},
doi = {10.1145/321796.321811},
abstract = {The string-to-string correction problem is to determine the distance between two strings as measured by the minimum cost sequence of “edit operations” needed to change the one string into the other. The edit operations investigated allow changing one symbol of a string into another single symbol, deleting one symbol from a string, or inserting a single symbol into a string. An algorithm is presented which solves this problem in time proportional to the product of the lengths of the two strings. Possible applications are to the problems of automatic spelling correction and determining the longest subsequence of characters common to two strings.},
journal = {J. ACM},
month = {jan},
pages = {168–173},
numpages = {6}
}


@article{levenshteinComplexityProof,
	title = {Edit {Distance} {Cannot} {Be} {Computed} in {Strongly} {Subquadratic} {Time} (unless {SETH} is false)},
	url = {http://arxiv.org/abs/1412.0348},
	abstract = {The edit distance (a.k.a. the Levenshtein distance) between two strings is defined as the minimum number of insertions, deletions or substitutions of symbols needed to transform one string into another. The problem of computing the edit distance between two strings is a classical computational task, with a well-known algorithm based on dynamic programming. Unfortunately, all known algorithms for this problem run in nearly quadratic time. In this paper we provide evidence that the near-quadratic running time bounds known for the problem of computing edit distance might be tight. Specifically, we show that, if the edit distance can be computed in time $O(n{\textasciicircum}\{2-{\textbackslash}delta\})$ for some constant ${\textbackslash}delta{\textgreater}0$, then the satisfiability of conjunctive normal form formulas with \$N\$ variables and \$M\$ clauses can be solved in time $M{\textasciicircum}\{O(1)\} 2{\textasciicircum}\{(1-{\textbackslash}epsilon)N\}$ for a constant ${\textbackslash}epsilon{\textgreater}0$. The latter result would violate the Strong Exponential Time Hypothesis, which postulates that such algorithms do not exist.},
	urldate = {2022-03-21},
	journal = {arXiv:1412.0348 [cs]},
	author = {Backurs, Arturs and Indyk, Piotr},
	month = aug,
	year = {2017},
	note = {arXiv: 1412.0348},
	keywords = {Computer Science - Computational Complexity, Computer Science - Data Structures and Algorithms},
}

@article{fastestLevenshteinAlg,
title = {A faster algorithm computing string edit distances},
journal = {Journal of Computer and System Sciences},
volume = {20},
number = {1},
pages = {18-31},
year = {1980},
issn = {0022-0000},
doi = {https://doi.org/10.1016/0022-0000(80)90002-1},
url = {https://www.sciencedirect.com/science/article/pii/0022000080900021},
author = {William J. Masek and Michael S. Paterson},
abstract = {The edit distance between two character strings can be defined as the minimum cost of a sequence of editing operations which transforms one string into the other. The operations we admit are deleting, inserting and replacing one symbol at a time, with possibly different costs for each of these operations. The problem of finding the longest common subsequence of two strings is a special case of the problem of computing edit distances. We describe an algorithm for computing the edit distance between two strings of length n and m, n ⪖ m, which requires O(n · max(1, mlog n)) steps whenever the costs of edit operations are integral multiples of a single positive real number and the alphabet for the strings is finite. These conditions are necessary for the algorithm to achieve the time bound.}
}

@article{damerauLevenshtein,
author = {Damerau, Fred J.},
title = {A Technique for Computer Detection and Correction of Spelling Errors},
year = {1964},
issue_date = {March 1964},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {0001-0782},
url = {https://doi.org/10.1145/363958.363994},
doi = {10.1145/363958.363994},
abstract = {The method described assumes that a word which cannot be found in a dictionary has at most one error, which might be a wrong, missing or extra letter or a single transposition. The unidentified input word is compared to the dictionary again, testing each time to see if the words match—assuming one of these errors occurred. During a test run on garbled text, correct identifications were made for over 95 percent of these error types.},
journal = {Commun. ACM},
month = {mar},
pages = {171–176},
numpages = {6}
}

@article{damerauLevenshteinDefinition,
author = {Boytsov, Leonid},
title = {Indexing Methods for Approximate Dictionary Searching: Comparative Analysis},
year = {2011},
issue_date = {2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
issn = {1084-6654},
url = {https://doi.org/10.1145/1963190.1963191},
doi = {10.1145/1963190.1963191},
abstract = {The primary goal of this article is to survey state-of-the-art indexing methods for approximate dictionary searching. To improve understanding of the field, we introduce a taxonomy that classifies all methods into direct methods and sequence-based filtering methods. We focus on infrequently updated dictionaries, which are used primarily for retrieval. Therefore, we consider indices that are optimized for retrieval rather than for update. The indices are assumed to be associative, that is, capable of storing and retrieving auxiliary information, such as string identifiers. All solutions are lossless and guarantee retrieval of strings within a specified edit distance k. Benchmark results are presented for the practically important cases of k=1, 2, and 3. We concentrate on natural language datasets, which include synthetic English and Russian dictionaries, as well as dictionaries of frequent words extracted from the ClueWeb09 collection. In addition, we carry out experiments with dictionaries containing DNA sequences. The article is concluded with a discussion of benchmark results and directions for future research.},
journal = {ACM J. Exp. Algorithmics},
month = {may},
articleno = {1.1},
numpages = {91},
keywords = {agrep, k-errata tree, approximate searching, q-gram, frequency distance, frequency vector trie, q-sample, Damerau-Levenshtein distance, metric trees, Levenshtein distance, neighborhood generation, trie, NR-grep}
}

@article{jaroDescription,
  title={Similarity measures},
  author={Naumann, Felix},
  journal={Information Systems},
  year={2013}
}


@article{jaroWinklerOriginalPaper,
author = { Matthew A.   Jaro },
title = {Advances in Record-Linkage Methodology as Applied to Matching the 1985 Census of Tampa, Florida},
journal = {Journal of the American Statistical Association},
volume = {84},
number = {406},
pages = {414-420},
year  = {1989},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.1989.10478785},
}

@misc{jaroWinklerTriangleLaw,
    title={Record {Linkage} {Algorithms} in F\# - {Extensions} to {Jaro-Winkler Distance} (Part 3)},
    author={Richard Minerich},
    year=2011,
    month=sep
}

@InProceedings{ngramDistanceImplementation,
author="Kondrak, Grzegorz",
editor="Consens, Mariano
and Navarro, Gonzalo",
title="N-Gram Similarity and Distance",
booktitle="String Processing and Information Retrieval",
year="2005",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="115--126",
abstract="In many applications, it is necessary to algorithmically quantify the similarity exhibited by two strings composed of symbols from a finite alphabet. Numerous string similarity measures have been proposed. Particularly well-known measures are based are edit distance and the length of the longest common subsequence. We develop a notion of n-gram similarity and distance. We show that edit distance and the length of the longest common subsequence are special cases of n-gram distance and similarity, respectively. We provide formal, recursive definitions of n-gram similarity and distance, together with efficient algorithms for computing them. We formulate a family of word similarity measures based on n-grams, and report the results of experiments that suggest that the new measures outperform their unigram equivalents.",
isbn="978-3-540-32241-2"
}

@INPROCEEDINGS{lcsFormalDefinition,
  author={Bergroth, L. and Hakonen, H. and Raita, T.},
  booktitle={Proceedings Seventh International Symposium on String Processing and Information Retrieval. SPIRE 2000}, 
  title={A survey of longest common subsequence algorithms}, 
  year={2000},
  volume={},
  number={},
  pages={39-48},
  doi={10.1109/SPIRE.2000.878178}}

@article{ngramBio,
title = {n-Gram-based classification and unsupervised hierarchical clustering of genome sequences},
journal = {Computer Methods and Programs in Biomedicine},
volume = {81},
number = {2},
pages = {137-153},
year = {2006},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2005.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0169260705002361},
author = {Andrija Tomović and Predrag Janičić and Vlado Kešelj},
keywords = {-Gram, Classification, Hierarchical clustering, Genome sequence},
abstract = {In this paper we address the problem of automated classification of isolates, i.e., the problem of determining the family of genomes to which a given genome belongs. Additionally, we address the problem of automated unsupervised hierarchical clustering of isolates according only to their statistical substring properties. For both of these problems we present novel algorithms based on nucleotide n-grams, with no required preprocessing steps such as sequence alignment. Results obtained experimentally are very positive and suggest that the proposed techniques can be successfully used in a variety of related problems. The reported experiments demonstrate better performance than some of the state-of-the-art methods. We report on a new distance measure between n-gram profiles, which shows superior performance compared to many other measures, including commonly used Euclidean distance.}
}

@book{computingPatternsInStrings,
    title={{Computing Patterns In Strings}},
    author={William Smyth},
    publisher={Pearson Education},
    year=2003
}

@article{jensenShannonTrees,
title = {A bounded distance metric for comparing tree structure},
journal = {Information Systems},
volume = {36},
number = {4},
pages = {748-764},
year = {2011},
note = {Selected Papers from the 2nd International Workshop on Similarity Search and Applications SISAP 2009},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2010.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0306437910001353},
author = {Richard Connor and Fabio Simeoni and Michael Iakovos and Robert Moss},
keywords = {Unordered tree, Tree comparison, Distance metric, Algorithmic information theory, Information content, Information distance, Entropy},
abstract = {Comparing tree-structured data for structural similarity is a recurring theme and one on which much effort has been spent. Most approaches so far are grounded, implicitly or explicitly, in algorithmic information theory, being approximations to an information distance derived from Kolmogorov complexity. In this paper we propose a novel complexity metric, also grounded in information theory, but calculated via Shannon's entropy equations. This is used to formulate a directly and efficiently computable metric for the structural difference between unordered trees. The paper explains the derivation of the metric in terms of information theory, and proves the essential property that it is a distance metric. The property of boundedness means that the metric can be used in contexts such as clustering, where second-order comparisons are required. The distance metric property means that the metric can be used in the context of similarity search and metric spaces in general, allowing trees to be indexed and stored within this domain. We are not aware of any other tree similarity metric with these properties.}
}

@article{richardsPaper,
    title={Modelling {String} {Structure} in {Vector} {Spaces}},
    author={Richard Connor and Alan Dearle and Lucia Vadicamo},
    year=2019,
    month=aug,
    abstract={Searching for similar strings is an important and frequent database task both in terms of human interactions and in absolute worldwide CPU utilisation. A wealth of metric functions for string comparison exist. However, with respect to the wide range of classification and other techniques known within vector spaces, such metrics allow only a very restricted range of techniques. To counter this restriction, various strategies have been used for mapping string spaces into vector spaces, approximating the string distances within the mapped space and therefore allowing vector space techniques to be used. In previous work we have developed a novel technique for mapping metric spaces into vector spaces, which can therefore be applied for this purpose. In this paper we evaluate this technique in the context of string spaces, and compare it to other published techniques for mapping strings to vectors. We use a publicly available English lexicon as our experimental data set, and test two different string metrics over it for each vector mapping. We find that our novel technique considerably outperforms previously used technique in preserving the actual distance.},
    url={http://hdl.handle.net/1893/29995}
}

@manual{mui,
    title={Material User Interface},
    author={{MUI core Team}},
    year=2022,
    url={https://mui.com/}
}

@manual{electronReactBoilerPlate,
    title={Native Modules in Electron-React-Boilerplate},
    year=2022,
    url={https://electron-react-boilerplate.js.org/docs/native-modules/},
    author={{Electron React Boilerplate}}
}

@inproceedings{mlStringMatching,
author = {Lu, Jiaheng and Lin, Chunbin and Wang, Jin and Li, Chen},
title = {Synergy of Database Techniques and Machine Learning Models for String Similarity Search and Join},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3360319},
doi = {10.1145/3357384.3360319},
abstract = {String data is ubiquitous and string similarity search and join are critical to the applications of information retrieval, data integration, data cleaning, and also big data analytics. To support these operations, many techniques in the database and machine learning areas have been proposed independently. More precisely, in the database research area, there are techniques based on the filtering-and-verification framework that can not only achieve a high performance, but also provide guaranteed quality of results for given similarity functions. In the machine learning research area, string similarity processing is modeled as a problem of identifying similar text records; Specifically, the deep learning approaches use embedding techniques that map text to a low-dimensional continuous vector space. In this tutorial, we review a number of studies of string similarity search and join in these two research areas. We divide the studies in each area into different categories. For each category, we provide a comprehensive review of the relevant works, and present the details of these solutions. We conclude this tutorial by pinpointing promising directions for future work to combine techniques in these two areas.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {2975–2976},
numpages = {2},
keywords = {databases, machine learning, string similarity join, string similarity search, data integration},
location = {Beijing, China},
series = {CIKM '19}
}

  


